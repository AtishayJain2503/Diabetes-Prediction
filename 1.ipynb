{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"/Users/sakshamjain/Desktop/Projects/JAIN-WIN/widsdatathon2021/TrainingWiDS2021.csv\", index_col=0)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:20,.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1_bilirubin_max                             92.09\n",
      "h1_bilirubin_min                             92.09\n",
      "h1_albumin_max                               91.43\n",
      "h1_albumin_min                               91.43\n",
      "h1_lactate_max                               91.02\n",
      "h1_lactate_min                               91.02\n",
      "h1_pao2fio2ratio_min                         87.12\n",
      "h1_pao2fio2ratio_max                         87.12\n",
      "h1_arterial_ph_max                           82.86\n",
      "h1_arterial_ph_min                           82.86\n",
      "h1_arterial_pco2_min                         82.72\n",
      "h1_arterial_pco2_max                         82.72\n",
      "h1_arterial_po2_max                          82.55\n",
      "h1_arterial_po2_min                          82.55\n",
      "h1_hco3_max                                  81.74\n",
      "h1_hco3_min                                  81.74\n",
      "h1_wbc_max                                   81.43\n",
      "h1_wbc_min                                   81.43\n",
      "h1_calcium_min                               81.38\n",
      "h1_calcium_max                               81.38\n",
      "h1_platelets_min                             81.23\n",
      "h1_platelets_max                             81.23\n",
      "h1_bun_max                                   80.66\n",
      "h1_bun_min                                   80.66\n",
      "h1_diasbp_invasive_min                       80.54\n",
      "h1_diasbp_invasive_max                       80.54\n",
      "h1_sysbp_invasive_min                        80.52\n",
      "h1_sysbp_invasive_max                        80.52\n",
      "h1_creatinine_max                            80.51\n",
      "h1_creatinine_min                            80.51\n",
      "h1_mbp_invasive_min                          80.49\n",
      "h1_mbp_invasive_max                          80.49\n",
      "h1_hematocrit_min                            79.10\n",
      "h1_hematocrit_max                            79.10\n",
      "h1_hemaglobin_max                            78.97\n",
      "h1_hemaglobin_min                            78.97\n",
      "h1_sodium_max                                78.20\n",
      "h1_sodium_min                                78.20\n",
      "h1_potassium_min                             77.46\n",
      "h1_potassium_max                             77.46\n",
      "paco2_apache                                 76.62\n",
      "paco2_for_ph_apache                          76.62\n",
      "pao2_apache                                  76.62\n",
      "ph_apache                                    76.62\n",
      "fio2_apache                                  76.62\n",
      "d1_lactate_min                               73.38\n",
      "d1_lactate_max                               73.38\n",
      "d1_diasbp_invasive_max                       73.04\n",
      "d1_diasbp_invasive_min                       73.04\n",
      "d1_sysbp_invasive_max                        73.02\n",
      "d1_sysbp_invasive_min                        73.02\n",
      "d1_mbp_invasive_max                          72.89\n",
      "d1_mbp_invasive_min                          72.89\n",
      "d1_pao2fio2ratio_max                         71.71\n",
      "d1_pao2fio2ratio_min                         71.71\n",
      "d1_arterial_ph_min                           65.16\n",
      "d1_arterial_ph_max                           65.16\n",
      "d1_arterial_pco2_max                         64.89\n",
      "d1_arterial_pco2_min                         64.89\n",
      "d1_arterial_po2_max                          64.55\n",
      "d1_arterial_po2_min                          64.55\n",
      "bilirubin_apache                             63.43\n",
      "h1_inr_max                                   62.40\n",
      "d1_inr_min                                   62.40\n",
      "d1_inr_max                                   62.40\n",
      "h1_inr_min                                   62.40\n",
      "albumin_apache                               60.05\n",
      "d1_bilirubin_max                             58.96\n",
      "d1_bilirubin_min                             58.96\n",
      "h1_glucose_max                               57.68\n",
      "h1_glucose_min                               57.68\n",
      "d1_albumin_max                               54.86\n",
      "d1_albumin_min                               54.86\n",
      "urineoutput_apache                           48.53\n",
      "hospital_admit_source                        25.51\n",
      "h1_temp_min                                  22.82\n",
      "h1_temp_max                                  22.82\n",
      "wbc_apache                                   22.65\n",
      "hematocrit_apache                            20.56\n",
      "bun_apache                                   19.52\n",
      "creatinine_apache                            19.12\n",
      "sodium_apache                                18.84\n",
      "d1_hco3_max                                  15.40\n",
      "d1_hco3_min                                  15.40\n",
      "d1_platelets_min                             14.26\n",
      "d1_platelets_max                             14.26\n",
      "d1_wbc_min                                   13.39\n",
      "d1_wbc_max                                   13.39\n",
      "d1_calcium_min                               12.82\n",
      "d1_calcium_max                               12.82\n",
      "d1_hemaglobin_max                            12.47\n",
      "d1_hemaglobin_min                            12.47\n",
      "d1_hematocrit_max                            11.98\n",
      "d1_hematocrit_min                            11.98\n",
      "glucose_apache                               11.29\n",
      "d1_bun_min                                   10.55\n",
      "d1_bun_max                                   10.55\n",
      "h1_mbp_noninvasive_min                       10.22\n",
      "h1_mbp_noninvasive_max                       10.22\n",
      "d1_creatinine_max                            10.20\n",
      "d1_creatinine_min                            10.20\n",
      "d1_sodium_max                                10.20\n",
      "d1_sodium_min                                10.20\n",
      "d1_potassium_max                              9.64\n",
      "d1_potassium_min                              9.64\n",
      "h1_diasbp_noninvasive_min                     8.71\n",
      "h1_diasbp_noninvasive_max                     8.71\n",
      "h1_sysbp_noninvasive_max                      8.70\n",
      "h1_sysbp_noninvasive_min                      8.70\n",
      "d1_glucose_max                                6.33\n",
      "d1_glucose_min                                6.33\n",
      "temp_apache                                   5.08\n",
      "h1_mbp_min                                    5.02\n",
      "h1_mbp_max                                    5.02\n",
      "h1_resprate_min                               4.96\n",
      "h1_resprate_max                               4.96\n",
      "h1_spo2_max                                   4.80\n",
      "h1_spo2_min                                   4.80\n",
      "h1_diasbp_min                                 4.25\n",
      "h1_diasbp_max                                 4.25\n",
      "h1_sysbp_max                                  4.24\n",
      "h1_sysbp_min                                  4.24\n",
      "age                                           3.83\n",
      "d1_temp_max                                   3.45\n",
      "d1_temp_min                                   3.45\n",
      "bmi                                           3.45\n",
      "h1_heartrate_min                              3.13\n",
      "h1_heartrate_max                              3.13\n",
      "weight                                        2.66\n",
      "d1_mbp_noninvasive_max                        1.71\n",
      "d1_mbp_noninvasive_min                        1.71\n",
      "gcs_eyes_apache                               1.68\n",
      "gcs_verbal_apache                             1.68\n",
      "gcs_motor_apache                              1.68\n",
      "height                                        1.60\n",
      "apache_2_diagnosis                            1.29\n",
      "d1_diasbp_noninvasive_min                     1.26\n",
      "d1_diasbp_noninvasive_max                     1.26\n",
      "d1_sysbp_noninvasive_min                      1.25\n",
      "d1_sysbp_noninvasive_max                      1.25\n",
      "ethnicity                                     1.22\n",
      "apache_3j_diagnosis                           0.66\n",
      "resprate_apache                               0.62\n",
      "gcs_unable_apache                             0.54\n",
      "d1_resprate_min                               0.52\n",
      "d1_resprate_max                               0.52\n",
      "d1_spo2_min                                   0.41\n",
      "d1_spo2_max                                   0.41\n",
      "map_apache                                    0.32\n",
      "d1_mbp_max                                    0.25\n",
      "d1_mbp_min                                    0.25\n",
      "heart_rate_apache                             0.24\n",
      "d1_diasbp_min                                 0.21\n",
      "d1_diasbp_max                                 0.21\n",
      "d1_sysbp_min                                  0.21\n",
      "d1_sysbp_max                                  0.21\n",
      "d1_heartrate_max                              0.20\n",
      "d1_heartrate_min                              0.20\n",
      "icu_admit_source                              0.18\n",
      "gender                                        0.05\n",
      "cirrhosis                                     0.00\n",
      "hepatic_failure                               0.00\n",
      "leukemia                                      0.00\n",
      "lymphoma                                      0.00\n",
      "solid_tumor_with_metastasis                   0.00\n",
      "aids                                          0.00\n",
      "immunosuppression                             0.00\n",
      "encounter_id                                  0.00\n",
      "hospital_id                                   0.00\n",
      "ventilated_apache                             0.00\n",
      "intubated_apache                              0.00\n",
      "arf_apache                                    0.00\n",
      "apache_post_operative                         0.00\n",
      "readmission_status                            0.00\n",
      "pre_icu_los_days                              0.00\n",
      "icu_type                                      0.00\n",
      "icu_stay_type                                 0.00\n",
      "icu_id                                        0.00\n",
      "elective_surgery                              0.00\n",
      "diabetes_mellitus                             0.00\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of missing values for each column\n",
    "missing_percentages = data.isnull().sum() * 100 / len(data)\n",
    "missing_percentages_sorted = missing_percentages.sort_values(ascending=False)\n",
    "print(missing_percentages_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['encounter_id', 'hospital_id', 'age', 'bmi', 'elective_surgery', 'ethnicity', 'gender', 'height', 'hospital_admit_source', 'icu_admit_source',\n",
      "       ...\n",
      "       'd1_arterial_po2_max', 'd1_arterial_po2_min', 'aids', 'cirrhosis', 'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 'diabetes_mellitus'], dtype='object', length=125)\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with more than 75% missing values and drop them\n",
    "columns_to_drop = missing_percentages[missing_percentages > 70].index\n",
    "data_cleaned = data.drop(columns=columns_to_drop)\n",
    "data_cleaned.to_csv('1.csv', index=False)\n",
    "print(data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethnicity: 6\n",
      "gender: 2\n",
      "hospital_admit_source: 15\n",
      "icu_admit_source: 5\n",
      "icu_stay_type: 3\n",
      "icu_type: 8\n"
     ]
    }
   ],
   "source": [
    "# Identifying categorical columns\n",
    "categorical_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_columns\n",
    "for col in ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']:\n",
    "    print(f\"{col}: {data[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     30574\n",
      "           1       0.67      0.34      0.45      8474\n",
      "\n",
      "    accuracy                           0.82     39048\n",
      "   macro avg       0.75      0.65      0.67     39048\n",
      "weighted avg       0.80      0.82      0.80     39048\n",
      "\n",
      "AUC ROC: 0.8247452093504967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('1.csv')\n",
    "\n",
    "# Binary encoding for 'gender'\n",
    "data['gender'] = data['gender'].astype('category').cat.codes\n",
    "\n",
    "# One-hot encoding for other categorical columns\n",
    "categorical_columns = ['ethnicity', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "data = pd.get_dummies(data, columns=categorical_columns)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X = data.drop(['diabetes_mellitus', 'encounter_id', 'hospital_id'], axis=1)\n",
    "y = data['diabetes_mellitus'].astype(int)  # Ensure the target is integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Imputation on training and test sets separately\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=X_train_imputed.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed), columns=X_test_imputed.columns)  \n",
    "\n",
    "# Training the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "y_prob = rf_classifier.predict_proba(X_test_scaled)[:, 1]  # probabilities for AUC\n",
    "\n",
    "# Metrics\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(classification_rep)\n",
    "print('AUC ROC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MICE imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     30574\n",
      "           1       0.66      0.35      0.46      8474\n",
      "\n",
      "    accuracy                           0.82     39048\n",
      "   macro avg       0.75      0.65      0.67     39048\n",
      "weighted avg       0.80      0.82      0.80     39048\n",
      "\n",
      "AUC ROC: 0.8235569985397327\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.experimental import enable_iterative_imputer  # This line enables the experimental features\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('1.csv')\n",
    "\n",
    "# Binary encoding for 'gender'\n",
    "data['gender'] = data['gender'].astype('category').cat.codes\n",
    "\n",
    "# One-hot encoding for other categorical columns\n",
    "categorical_columns = ['ethnicity', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "data = pd.get_dummies(data, columns=categorical_columns)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X = data.drop(['diabetes_mellitus', 'encounter_id', 'hospital_id'], axis=1)\n",
    "y = data['diabetes_mellitus'].astype(int)  # Ensure the target is integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# MICE imputation on training and test sets separately\n",
    "mice_imputer = IterativeImputer(random_state=42,max_iter=10,n_nearest_features=10)\n",
    "X_train_imputed = pd.DataFrame(mice_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_imputed = pd.DataFrame(mice_imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=X_train_imputed.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed), columns=X_test_imputed.columns)\n",
    "\n",
    "# Training the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "y_prob = rf_classifier.predict_proba(X_test_scaled)[:, 1]  # probabilities for AUC\n",
    "\n",
    "# Metrics\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(classification_rep)\n",
    "print('AUC ROC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SICE imputer (mean of MICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Imputing Data:   0%|          | 0/10 [00:00<?, ?imputation/s]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  10%|█         | 1/10 [00:22<03:21, 22.35s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  20%|██        | 2/10 [00:44<02:59, 22.44s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  30%|███       | 3/10 [01:06<02:34, 22.14s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  40%|████      | 4/10 [01:28<02:12, 22.00s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  50%|█████     | 5/10 [01:50<01:50, 22.19s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  60%|██████    | 6/10 [02:13<01:28, 22.14s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  70%|███████   | 7/10 [02:35<01:06, 22.13s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  80%|████████  | 8/10 [02:57<00:44, 22.07s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data:  90%|█████████ | 9/10 [03:19<00:22, 22.05s/imputation]/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "Imputing Data: 100%|██████████| 10/10 [03:41<00:00, 22.18s/imputation]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     30574\n",
      "           1       0.67      0.35      0.46      8474\n",
      "\n",
      "    accuracy                           0.82     39048\n",
      "   macro avg       0.75      0.65      0.68     39048\n",
      "weighted avg       0.80      0.82      0.80     39048\n",
      "\n",
      "AUC ROC: 0.8219820252480511\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('1.csv')\n",
    "\n",
    "# Binary encoding for 'gender'\n",
    "data['gender'] = data['gender'].astype('category').cat.codes\n",
    "\n",
    "# One-hot encoding for other categorical columns\n",
    "categorical_columns = ['ethnicity', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "data = pd.get_dummies(data, columns=categorical_columns)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X = data.drop(['diabetes_mellitus', 'encounter_id', 'hospital_id'], axis=1)\n",
    "y = data['diabetes_mellitus'].astype(int)  # Ensure the target is integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the number of imputations\n",
    "m = 10  # number of multiple imputations\n",
    "\n",
    "# Prepare an empty array to store each set of imputations for both train and test data\n",
    "imputed_X_train = np.zeros((m, *X_train.shape))\n",
    "imputed_X_test = np.zeros((m, *X_test.shape))\n",
    "\n",
    "# Run MICE m times for both train and test sets with progress bar\n",
    "for i in tqdm(range(m), desc=\"Imputing Data\", unit=\"imputation\"):\n",
    "    imputer = IterativeImputer(random_state=i, max_iter=10, sample_posterior=False,n_nearest_features=10)\n",
    "    imputed_X_train[i] = imputer.fit_transform(X_train)\n",
    "    imputed_X_test[i] = imputer.transform(X_test)\n",
    "\n",
    "# Calculate the mean of the imputations\n",
    "mean_imputed_X_train = np.mean(imputed_X_train, axis=0)\n",
    "mean_imputed_X_test = np.mean(imputed_X_test, axis=0)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_imputed = pd.DataFrame(mean_imputed_X_train, columns=X_train.columns)\n",
    "X_test_imputed = pd.DataFrame(mean_imputed_X_test, columns=X_test.columns)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Training the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "y_prob = rf_classifier.predict_proba(X_test_scaled)[:, 1]  # probabilities for AUC\n",
    "\n",
    "# Metrics\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(classification_rep)\n",
    "print('AUC ROC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MICE with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakshamjain/tensorflow69/env/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     30574\n",
      "           1       0.57      0.56      0.56      8474\n",
      "\n",
      "    accuracy                           0.81     39048\n",
      "   macro avg       0.72      0.72      0.72     39048\n",
      "weighted avg       0.81      0.81      0.81     39048\n",
      "\n",
      "AUC ROC: 0.8290821798712168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.experimental import enable_iterative_imputer  # This line enables the experimental features\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('1.csv')\n",
    "\n",
    "# Binary encoding for 'gender'\n",
    "data['gender'] = data['gender'].astype('category').cat.codes\n",
    "\n",
    "# One-hot encoding for other categorical columns\n",
    "categorical_columns = ['ethnicity', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "data = pd.get_dummies(data, columns=categorical_columns)\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X = data.drop(['diabetes_mellitus', 'encounter_id', 'hospital_id'], axis=1)\n",
    "y = data['diabetes_mellitus'].astype(int)  # Ensure the target is integer\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# MICE imputation on training and test sets separately\n",
    "mice_imputer = IterativeImputer(random_state=42,max_iter=10,n_nearest_features=10)\n",
    "X_train_imputed = pd.DataFrame(mice_imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test_imputed = pd.DataFrame(mice_imputer.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed), columns=X_train_imputed.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed), columns=X_test_imputed.columns)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Training the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "y_prob = rf_classifier.predict_proba(X_test_scaled)[:, 1]  # probabilities for AUC\n",
    "\n",
    "# Metrics\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(classification_rep)\n",
    "print('AUC ROC:', roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAIN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import normalization, renormalization, rounding\n",
    "from utils import xavier_init\n",
    "from utils import binary_sampler, uniform_sampler, sample_batch_index\n",
    "\n",
    "\n",
    "def gain (data_x, gain_parameters):\n",
    "  '''Impute missing values in data_x\n",
    "  \n",
    "  Args:\n",
    "    - data_x: original data with missing values\n",
    "    - gain_parameters: GAIN network parameters:\n",
    "      - batch_size: Batch size\n",
    "      - hint_rate: Hint rate\n",
    "      - alpha: Hyperparameter\n",
    "      - iterations: Iterations\n",
    "      \n",
    "  Returns:\n",
    "    - imputed_data: imputed data\n",
    "  '''\n",
    "  # Define mask matrix\n",
    "  data_m = 1-np.isnan(data_x)\n",
    "  \n",
    "  # System parameters\n",
    "  batch_size = gain_parameters['batch_size']\n",
    "  hint_rate = gain_parameters['hint_rate']\n",
    "  alpha = gain_parameters['alpha']\n",
    "  iterations = gain_parameters['iterations']\n",
    "  \n",
    "  # Other parameters\n",
    "  no, dim = data_x.shape\n",
    "  \n",
    "  # Hidden state dimensions\n",
    "  h_dim = int(dim)\n",
    "  \n",
    "  # Normalization\n",
    "  norm_data, norm_parameters = normalization(data_x)\n",
    "  norm_data_x = np.nan_to_num(norm_data, 0)\n",
    "  \n",
    "  ## GAIN architecture   \n",
    "  # Input placeholders\n",
    "  # Data vector\n",
    "  X = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Mask vector \n",
    "  M = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Hint vector\n",
    "  H = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  \n",
    "  # Discriminator variables\n",
    "  D_W1 = tf.Variable(xavier_init([dim*2, h_dim])) # Data + Hint as inputs\n",
    "  D_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  D_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  D_b3 = tf.Variable(tf.zeros(shape = [dim]))  # Multi-variate outputs\n",
    "  \n",
    "  theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "  \n",
    "  #Generator variables\n",
    "  # Data + Mask as inputs (Random noise is in missing components)\n",
    "  G_W1 = tf.Variable(xavier_init([dim*2, h_dim]))  \n",
    "  G_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  G_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  G_b3 = tf.Variable(tf.zeros(shape = [dim]))\n",
    "  \n",
    "  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "  \n",
    "  ## GAIN functions\n",
    "  # Generator\n",
    "  def generator(x,m):\n",
    "    # Concatenate Mask and Data\n",
    "    inputs = tf.concat(values = [x, m], axis = 1) \n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
    "    # MinMax normalized output\n",
    "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) \n",
    "    return G_prob\n",
    "      \n",
    "  # Discriminator\n",
    "  def discriminator(x, h):\n",
    "    # Concatenate Data and Hint\n",
    "    inputs = tf.concat(values = [x, h], axis = 1) \n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob\n",
    "  \n",
    "  ## GAIN structure\n",
    "  # Generator\n",
    "  G_sample = generator(X, M)\n",
    " \n",
    "  # Combine with observed data\n",
    "  Hat_X = X * M + G_sample * (1-M)\n",
    "  \n",
    "  # Discriminator\n",
    "  D_prob = discriminator(Hat_X, H)\n",
    "  \n",
    "  ## GAIN loss\n",
    "  D_loss_temp = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) \\\n",
    "                                + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "  \n",
    "  G_loss_temp = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "  \n",
    "  MSE_loss = \\\n",
    "  tf.reduce_mean((M * X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "  \n",
    "  D_loss = D_loss_temp\n",
    "  G_loss = G_loss_temp + alpha * MSE_loss \n",
    "  \n",
    "  ## GAIN solver\n",
    "  D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "  G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "  \n",
    "  ## Iterations\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "  # Start Iterations\n",
    "  for it in tqdm(range(iterations)):    \n",
    "      \n",
    "    # Sample batch\n",
    "    batch_idx = sample_batch_index(no, batch_size)\n",
    "    X_mb = norm_data_x[batch_idx, :]  \n",
    "    M_mb = data_m[batch_idx, :]  \n",
    "    # Sample random vectors  \n",
    "    Z_mb = uniform_sampler(0, 0.01, batch_size, dim) \n",
    "    # Sample hint vectors\n",
    "    H_mb_temp = binary_sampler(hint_rate, batch_size, dim)\n",
    "    H_mb = M_mb * H_mb_temp\n",
    "      \n",
    "    # Combine random vectors with observed vectors\n",
    "    X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss_temp], \n",
    "                              feed_dict = {M: M_mb, X: X_mb, H: H_mb})\n",
    "    _, G_loss_curr, MSE_loss_curr = \\\n",
    "    sess.run([G_solver, G_loss_temp, MSE_loss],\n",
    "             feed_dict = {X: X_mb, M: M_mb, H: H_mb})\n",
    "            \n",
    "  ## Return imputed data      \n",
    "  Z_mb = uniform_sampler(0, 0.01, no, dim) \n",
    "  M_mb = data_m\n",
    "  X_mb = norm_data_x          \n",
    "  X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "  imputed_data = sess.run([G_sample], feed_dict = {X: X_mb, M: M_mb})[0]\n",
    "  \n",
    "  imputed_data = data_m * norm_data_x + (1-data_m) * imputed_data\n",
    "  \n",
    "  # Renormalization\n",
    "  imputed_data = renormalization(imputed_data, norm_parameters)  \n",
    "  \n",
    "  # Rounding\n",
    "  imputed_data = rounding(imputed_data, data_x)  \n",
    "          \n",
    "  return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('1.csv')\n",
    "data = data.drop(['encounter_id', 'hospital_id'], axis=1)\n",
    "data['gender'] = data['gender'].astype('category').cat.codes\n",
    "categorical_columns = ['ethnicity', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "data = pd.get_dummies(data, columns=categorical_columns)\n",
    "data = data.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "\n",
    "# Remove rows with no missing values and save them as test data\n",
    "clean_data = data.dropna()\n",
    "clean_data.to_csv('test_data_no_missing_values.csv', index=False)\n",
    "\n",
    "# Find the rest of the data by excluding the clean_data\n",
    "rest_data = data.merge(clean_data.drop_duplicates(), on=data.columns.tolist(), \n",
    "                       how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n",
    "\n",
    "# Save the rest of the data as training data\n",
    "rest_data.to_csv('training_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 16:10:25.865380: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-24 16:10:25.865400: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-08-24 16:10:25.878031: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]2024-08-24 16:10:25.978396: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-08-24 16:10:26.115845: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "100%|██████████| 10000/10000 [01:42<00:00, 97.88it/s]\n",
      "2024-08-24 16:12:08.435960: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('training_data.csv')\n",
    "target = data['diabetes_mellitus']  \n",
    "features = data.drop(columns=['diabetes_mellitus']) \n",
    "\n",
    "data_x = features.to_numpy()\n",
    "\n",
    "# Set the GAIN parameters\n",
    "gain_parameters = {\n",
    "    'batch_size': 128,\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 10000\n",
    "}\n",
    "\n",
    "# Impute missing data using GAIN\n",
    "imputed_data = gain(data_x, gain_parameters)\n",
    "\n",
    "# Convert the imputed data back to a pandas DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=features.columns)\n",
    "\n",
    "# Reattach the target variable\n",
    "imputed_df['diabetes_mellitus'] = target\n",
    "\n",
    "# Save the imputed dataset to a new CSV file\n",
    "imputed_df.to_csv('training_data_imputed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
