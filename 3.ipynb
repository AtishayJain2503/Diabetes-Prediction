{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using GAN to impute data on training set, test set is non-missing values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import normalization, renormalization, rounding\n",
    "from utils import xavier_init\n",
    "from utils import binary_sampler, uniform_sampler, sample_batch_index\n",
    "\n",
    "\n",
    "def gain (data_x, gain_parameters,model_path):\n",
    "  '''Impute missing values in data_x\n",
    "  \n",
    "  Args:\n",
    "    - data_x: original data with missing values\n",
    "    - gain_parameters: GAIN network parameters:\n",
    "      - batch_size: Batch size\n",
    "      - hint_rate: Hint rate\n",
    "      - alpha: Hyperparameter\n",
    "      - iterations: Iterations\n",
    "      \n",
    "  Returns:\n",
    "    - imputed_data: imputed data\n",
    "  '''\n",
    "  # Define mask matrix\n",
    "  data_m = 1-np.isnan(data_x)\n",
    "  \n",
    "  # System parameters\n",
    "  batch_size = gain_parameters['batch_size']\n",
    "  hint_rate = gain_parameters['hint_rate']\n",
    "  alpha = gain_parameters['alpha']\n",
    "  iterations = gain_parameters['iterations']\n",
    "  \n",
    "  # Other parameters\n",
    "  no, dim = data_x.shape\n",
    "  \n",
    "  # Hidden state dimensions\n",
    "  h_dim = int(dim)\n",
    "  \n",
    "  # Normalization\n",
    "  norm_data, norm_parameters = normalization(data_x)\n",
    "  norm_data_x = np.nan_to_num(norm_data, 0)\n",
    "  \n",
    "  ## GAIN architecture   \n",
    "  # Input placeholders\n",
    "  # Data vector\n",
    "  X = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Mask vector \n",
    "  M = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  # Hint vector\n",
    "  H = tf.placeholder(tf.float32, shape = [None, dim])\n",
    "  \n",
    "  # Discriminator variables\n",
    "  D_W1 = tf.Variable(xavier_init([dim*2, h_dim])) # Data + Hint as inputs\n",
    "  D_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  D_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  D_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  D_b3 = tf.Variable(tf.zeros(shape = [dim]))  # Multi-variate outputs\n",
    "  \n",
    "  theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "  \n",
    "  #Generator variables\n",
    "  # Data + Mask as inputs (Random noise is in missing components)\n",
    "  G_W1 = tf.Variable(xavier_init([dim*2, h_dim]))  \n",
    "  G_b1 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "  G_b2 = tf.Variable(tf.zeros(shape = [h_dim]))\n",
    "  \n",
    "  G_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "  G_b3 = tf.Variable(tf.zeros(shape = [dim]))\n",
    "  \n",
    "  theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]\n",
    "  \n",
    "  ## GAIN functions\n",
    "  # Generator\n",
    "  def generator(x,m):\n",
    "    # Concatenate Mask and Data\n",
    "    inputs = tf.concat(values = [x, m], axis = 1) \n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
    "    # MinMax normalized output\n",
    "    G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) \n",
    "    return G_prob\n",
    "      \n",
    "  # Discriminator\n",
    "  def discriminator(x, h):\n",
    "    # Concatenate Data and Hint\n",
    "    inputs = tf.concat(values = [x, h], axis = 1) \n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)  \n",
    "    D_h2 = tf.nn.relu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob\n",
    "  \n",
    "  ## GAIN structure\n",
    "  # Generator\n",
    "  G_sample = generator(X, M)\n",
    " \n",
    "  # Combine with observed data\n",
    "  Hat_X = X * M + G_sample * (1-M)\n",
    "  \n",
    "  # Discriminator\n",
    "  D_prob = discriminator(Hat_X, H)\n",
    "  \n",
    "  ## GAIN loss\n",
    "  D_loss_temp = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) \\\n",
    "                                + (1-M) * tf.log(1. - D_prob + 1e-8)) \n",
    "  \n",
    "  G_loss_temp = -tf.reduce_mean((1-M) * tf.log(D_prob + 1e-8))\n",
    "  \n",
    "  MSE_loss = \\\n",
    "  tf.reduce_mean((M * X - M * G_sample)**2) / tf.reduce_mean(M)\n",
    "  \n",
    "  D_loss = D_loss_temp\n",
    "  G_loss = G_loss_temp + alpha * MSE_loss \n",
    "  \n",
    "  ## GAIN solver\n",
    "  D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "  G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "  saver = tf.train.Saver()\n",
    "  \n",
    "  ## Iterations\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "   \n",
    "  # Start Iterations\n",
    "  for it in tqdm(range(iterations)):    \n",
    "      \n",
    "    # Sample batch\n",
    "    batch_idx = sample_batch_index(no, batch_size)\n",
    "    X_mb = norm_data_x[batch_idx, :]  \n",
    "    M_mb = data_m[batch_idx, :]  \n",
    "    # Sample random vectors  \n",
    "    Z_mb = uniform_sampler(0, 0.01, batch_size, dim) \n",
    "    # Sample hint vectors\n",
    "    H_mb_temp = binary_sampler(hint_rate, batch_size, dim)\n",
    "    H_mb = M_mb * H_mb_temp\n",
    "      \n",
    "    # Combine random vectors with observed vectors\n",
    "    X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss_temp], \n",
    "                              feed_dict = {M: M_mb, X: X_mb, H: H_mb})\n",
    "    _, G_loss_curr, MSE_loss_curr = \\\n",
    "    sess.run([G_solver, G_loss_temp, MSE_loss],\n",
    "             feed_dict = {X: X_mb, M: M_mb, H: H_mb})\n",
    "  saver.save(sess, model_path)\n",
    "            \n",
    "  ## Return imputed data      \n",
    "  Z_mb = uniform_sampler(0, 0.01, no, dim) \n",
    "  M_mb = data_m\n",
    "  X_mb = norm_data_x          \n",
    "  X_mb = M_mb * X_mb + (1-M_mb) * Z_mb \n",
    "      \n",
    "  imputed_data = sess.run([G_sample], feed_dict = {X: X_mb, M: M_mb})[0]\n",
    "  \n",
    "  imputed_data = data_m * norm_data_x + (1-data_m) * imputed_data\n",
    "  \n",
    "  # Renormalization\n",
    "  imputed_data = renormalization(imputed_data, norm_parameters)  \n",
    "  \n",
    "  # Rounding\n",
    "  imputed_data = rounding(imputed_data, data_x)  \n",
    "          \n",
    "  return imputed_data, norm_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_trained_model(test_data, model_path, norm_parameters):\n",
    "    # Dimensions of data\n",
    "    no, dim = test_data.shape\n",
    "    h_dim = int(dim)\n",
    "    \n",
    "    # Rebuild the graph\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Define placeholders\n",
    "    X = tf.placeholder(tf.float32, shape=[None, dim])\n",
    "    M = tf.placeholder(tf.float32, shape=[None, dim])\n",
    "\n",
    "    # Redefine the generator and discriminator with exact architecture from training\n",
    "    # Generator variables\n",
    "    G_W1 = tf.Variable(xavier_init([dim*2, h_dim]))  # Dimensions need to be defined or imported\n",
    "    G_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "    G_W2 = tf.Variable(xavier_init([h_dim, h_dim]))\n",
    "    G_b2 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "    G_W3 = tf.Variable(xavier_init([h_dim, dim]))\n",
    "    G_b3 = tf.Variable(tf.zeros(shape=[dim]))\n",
    "\n",
    "    # Redefine generator function\n",
    "    def generator(x, m):\n",
    "        inputs = tf.concat(values=[x, m], axis=1) \n",
    "        G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "        G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)   \n",
    "        G_prob = tf.nn.sigmoid(tf.matmul(G_h2, G_W3) + G_b3) \n",
    "        return G_prob\n",
    "    \n",
    "\n",
    "    # Initialize the TensorFlow Saver object\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # Restore the model weights\n",
    "        saver.restore(sess, model_path)\n",
    "        \n",
    "        # Normalize test data\n",
    "        epsilon = 1e-10\n",
    "        norm_data_x = np.nan_to_num((test_data - norm_parameters['min_val']) / (norm_parameters['max_val'] - norm_parameters['min_val']+epsilon), 0)\n",
    "        data_m = 1-np.isnan(test_data)\n",
    "        \n",
    "        # Impute missing data\n",
    "        Z_mb = uniform_sampler(0, 0.01, test_data.shape[0], dim)\n",
    "        X_mb = data_m * norm_data_x + (1 - data_m) * Z_mb\n",
    "        \n",
    "        G_sample = generator(X, M)\n",
    "        imputed_data = sess.run(G_sample, feed_dict={X: X_mb, M: data_m})\n",
    "        final_data = np.where(np.isnan(test_data), imputed_data, test_data)\n",
    "        \n",
    "        final_data = renormalization(final_data, norm_parameters)\n",
    "        final_data = rounding(final_data, test_data)\n",
    "\n",
    "    return final_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 15:49:05.532555: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-24 15:49:05.532572: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-08-24 15:49:05.541394: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]2024-08-24 15:49:05.613346: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-08-24 15:49:05.745116: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
      "2024-08-24 15:49:05.931922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-08-24 15:49:06.142075: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv('1.csv')\n",
    "df.drop(['hospital_id'], axis=1, inplace=True)\n",
    "\n",
    "# Select features and target columns\n",
    "X = df.drop(['diabetes_mellitus', 'encounter_id'], axis=1)  # Dropping the target and identifier from the features\n",
    "y = df[['diabetes_mellitus', 'encounter_id']] \n",
    "\n",
    "X['gender'] = X['gender'].astype('category').cat.codes\n",
    "categorical_columns = ['ethnicity', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type']\n",
    "X = pd.get_dummies(X, columns=categorical_columns)\n",
    "X = X.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "gain_parameters = {\n",
    "    'batch_size': 128,\n",
    "    'hint_rate': 0.9,\n",
    "    'alpha': 100,\n",
    "    'iterations': 1\n",
    "}\n",
    "\n",
    "# Convert to numpy array for GAIN\n",
    "train_data = X_train.to_numpy()\n",
    "\n",
    "# Path to save model weights\n",
    "model_path = 'trained_model.ckpt'\n",
    "\n",
    "imputed_data, norm_parameters = gain(train_data, gain_parameters, model_path)\n",
    "\n",
    "imputed_df = pd.DataFrame(data=imputed_data, columns=X_train.columns)\n",
    "\n",
    "# Reset index to align the indices\n",
    "imputed_df.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add the target and identifier columns back\n",
    "imputed_df['diabetes_mellitus'] = y_train['diabetes_mellitus']\n",
    "imputed_df['encounter_id'] = y_train['encounter_id']\n",
    "\n",
    "# Save the DataFrame with added target column to a CSV file\n",
    "imputed_df.to_csv('imputed_data_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 15:49:12.488295: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-08-24 15:49:12.488318: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-08-24 15:49:12.491075: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-08-24 15:49:12.650337: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "imputed_test_data = impute_with_trained_model(X_test.to_numpy(), model_path, norm_parameters)\n",
    "imputed_test_df = pd.DataFrame(data=imputed_test_data, columns=X_test.columns)\n",
    "imputed_test_df.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "# Add the target and identifier columns back\n",
    "imputed_test_df['diabetes_mellitus'] = y_test['diabetes_mellitus']\n",
    "imputed_test_df['encounter_id'] = y_test['encounter_id']\n",
    "\n",
    "# Save the DataFrame with added target column to a CSV file\n",
    "imputed_test_df.to_csv('imputed_data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
