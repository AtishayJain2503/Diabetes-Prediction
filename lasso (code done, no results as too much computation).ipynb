{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***have lasso + lasso as a feature***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import special\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:20,.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_corr.csv')\n",
    "y_train = pd.read_csv('y_train_large.csv').squeeze()\n",
    "X_test = pd.read_csv('X_test_corr.csv')\n",
    "y_test = pd.read_csv('y_test_large.csv').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lightgbm(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - model (lgb.LGBMClassifier): Trained LightGBM model.\n",
    "    - roc_auc (float): ROC-AUC score on the test set.\n",
    "    - report (str): Classification report with precision, recall, and F1-score up to 4 decimals.\n",
    "    \"\"\"\n",
    "    # Initialize the LightGBM classifier with default parameters\n",
    "    model = lgb.LGBMClassifier(\n",
    "        random_state=69,        # For reproducibility\n",
    "        n_jobs=-1,\n",
    "        force_col_wise=True              # Utilize all available cores\n",
    "    )\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities for the positive class\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Predict class labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate ROC-AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Generate classification report with 4 decimal places\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(f\"ROC-AUC: {roc_auc:.8f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    return model, roc_auc, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_l1(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Performs feature selection using L1 Regularization (Lasso) without cross-validation \n",
    "    and evaluates a LightGBM model.\n",
    "\n",
    "    Returns:\n",
    "    - model (lgb.LGBMClassifier): Trained LightGBM model.\n",
    "    - roc_auc (float): ROC-AUC score on the test set.\n",
    "    - report (str): Classification report with precision, recall, and F1-score up to 4 decimals.\n",
    "    - selected_features (list): List of features selected by Lasso.\n",
    "    \"\"\"\n",
    "    # Step 1: Define a range of C values to explore (inverse of regularization strength)\n",
    "    #C_values = [0.01, 0.1, 1, 10, 100]\n",
    "    C_values= [0.1]\n",
    "    \n",
    "    best_auc = -1\n",
    "    best_C = None\n",
    "    \n",
    "    # Step 2: Manual Hyperparameter Tuning\n",
    "    for C in C_values:\n",
    "        # Initialize the Logistic Regression model with L1 penalty\n",
    "        lasso = LogisticRegression(\n",
    "            penalty='l1',\n",
    "            C=C,\n",
    "            solver='saga',  # Supports L1 penalty and is efficient for large datasets\n",
    "            max_iter=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Train the model on the entire training set\n",
    "        lasso.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict probabilities on the training set\n",
    "        y_train_pred_proba = lasso.predict_proba(X_train)[:, 1]\n",
    "        \n",
    "        # Calculate ROC-AUC on the training set\n",
    "        auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "        print(f\"Lasso with C={C}: Training ROC-AUC = {auc:.4f}\")\n",
    "        \n",
    "        # Update best C if current AUC is better\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_C = C\n",
    "    \n",
    "    print(f\"Best C value for Lasso: {best_C} with Training ROC-AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    # Step 3: Train Lasso on the entire training set with the best C\n",
    "    final_lasso = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        C=best_C,\n",
    "        solver='saga',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    final_lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 4: Extract the coefficients from the Lasso model\n",
    "    coefficients = final_lasso.coef_[0]\n",
    "    \n",
    "    # Step 5: Create a Series with feature names and their corresponding coefficients\n",
    "    feature_coefficients = pd.Series(coefficients, index=X_train.columns)\n",
    "    \n",
    "    # Step 6: Select features with non-zero coefficients\n",
    "    selected_features = feature_coefficients[feature_coefficients != 0].index.tolist()\n",
    "    \n",
    "    print(f\"Number of features selected by Lasso: {len(selected_features)}\")\n",
    "    \n",
    "    # Step 7: Reduce the training and testing sets to the selected features\n",
    "    X_train_lasso = X_train[selected_features]\n",
    "    X_test_lasso = X_test[selected_features]\n",
    "    \n",
    "    # Step 8: Evaluate the LightGBM model using the selected features\n",
    "    model, roc_auc, report = evaluate_lightgbm(X_train_lasso, y_train, X_test_lasso, y_test)\n",
    "    \n",
    "    return model, roc_auc, report, selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1, roc_auc_l1, report_l1, selected_features_l1 = evaluate_l1(X_train, y_train, X_test, y_test)\n",
    "print(report_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_elasticnet(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Performs feature selection using Elastic Net Regularization without cross-validation \n",
    "    and evaluates a LightGBM model.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training feature set (already scaled and encoded).\n",
    "    - y_train (pd.Series): Training target variable.\n",
    "    - X_test (pd.DataFrame): Testing feature set (already scaled and encoded).\n",
    "    - y_test (pd.Series): Testing target variable.\n",
    "\n",
    "    Returns:\n",
    "    - model (lgb.LGBMClassifier): Trained LightGBM model.\n",
    "    - roc_auc (float): ROC-AUC score on the test set.\n",
    "    - report (str): Classification report with precision, recall, and F1-score up to 4 decimals.\n",
    "    - selected_features (list): List of features selected by Elastic Net.\n",
    "    \"\"\"\n",
    "    # Step 1: Define a grid of hyperparameters to explore\n",
    "    C_values = [0.01, 0.1, 1, 10, 100]\n",
    "    l1_ratios = [0.1, 0.5, 0.7, 0.9, 1.0]  # 1.0 corresponds to pure L1 (Lasso)\n",
    "    \n",
    "    best_auc = -1\n",
    "    best_C = None\n",
    "    best_l1_ratio = None\n",
    "    \n",
    "    # Step 2: Manual Hyperparameter Tuning\n",
    "    for C in C_values:\n",
    "        for l1_ratio in l1_ratios:\n",
    "            # Initialize the Logistic Regression model with Elastic Net penalty\n",
    "            enet = LogisticRegression(\n",
    "                penalty='elasticnet',\n",
    "                C=C,\n",
    "                l1_ratio=l1_ratio,\n",
    "                solver='saga',  # Supports Elastic Net penalty\n",
    "                max_iter=1000,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Train the model on the entire training set\n",
    "            enet.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict probabilities on the training set\n",
    "            y_train_pred_proba = enet.predict_proba(X_train)[:, 1]\n",
    "            \n",
    "            # Calculate ROC-AUC on the training set\n",
    "            auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "            print(f\"Elastic Net with C={C}, l1_ratio={l1_ratio}: Training ROC-AUC = {auc:.4f}\")\n",
    "            \n",
    "            # Update best hyperparameters if current AUC is better\n",
    "            if auc > best_auc:\n",
    "                best_auc = auc\n",
    "                best_C = C\n",
    "                best_l1_ratio = l1_ratio\n",
    "    \n",
    "    print(f\"Best parameters for Elastic Net: C={best_C}, l1_ratio={best_l1_ratio} with Training ROC-AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    # Step 3: Train Elastic Net on the entire training set with the best hyperparameters\n",
    "    final_enet = LogisticRegression(\n",
    "        penalty='elasticnet',\n",
    "        C=best_C,\n",
    "        l1_ratio=best_l1_ratio,\n",
    "        solver='saga',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    final_enet.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 4: Extract the coefficients from the Elastic Net model\n",
    "    coefficients = final_enet.coef_[0]\n",
    "    \n",
    "    # Step 5: Create a Series with feature names and their corresponding coefficients\n",
    "    feature_coefficients = pd.Series(coefficients, index=X_train.columns)\n",
    "    \n",
    "    # Step 6: Select features with non-zero coefficients\n",
    "    selected_features = feature_coefficients[feature_coefficients != 0].index.tolist()\n",
    "    \n",
    "    print(f\"Number of features selected by Elastic Net: {len(selected_features)}\")\n",
    "    \n",
    "    # Step 7: Reduce the training and testing sets to the selected features\n",
    "    X_train_enet = X_train[selected_features]\n",
    "    X_test_enet = X_test[selected_features]\n",
    "    \n",
    "    # Step 8: Evaluate the LightGBM model using the selected features\n",
    "    model, roc_auc, report = evaluate_lightgbm(X_train_enet, y_train, X_test_enet, y_test)\n",
    "    \n",
    "    return model, roc_auc, report, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Evaluating Elastic Net Regularization ===\")\n",
    "model_enet, roc_auc_enet, report_enet, selected_features_enet = evaluate_elasticnet(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\n=== Elastic Net Classification Report ===\")\n",
    "print(report_enet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "def generate_lasso_predictions(X_train, y_train, X_test, best_C):\n",
    "    \"\"\"\n",
    "    Trains a Lasso model and generates predictions for training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training feature set (already scaled and encoded).\n",
    "    - y_train (pd.Series): Training target variable.\n",
    "    - X_test (pd.DataFrame): Testing feature set (already scaled and encoded).\n",
    "    - best_C (float): The best C value identified from previous feature selection.\n",
    "    \n",
    "    Returns:\n",
    "    - y_train_pred_proba (np.array): Predicted probabilities for the training set.\n",
    "    - y_test_pred_proba (np.array): Predicted probabilities for the testing set.\n",
    "    \"\"\"\n",
    "    # Initialize the Lasso model with the best C\n",
    "    lasso = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        C=best_C,\n",
    "        solver='saga',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train the Lasso model on the entire training set\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate predicted probabilities for both training and testing sets\n",
    "    y_train_pred_proba = lasso.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = lasso.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return y_train_pred_proba, y_test_pred_proba\n",
    "\n",
    "def add_lasso_predictions(X_train, X_test, y_train_pred_proba, y_test_pred_proba):\n",
    "    \"\"\"\n",
    "    Adds Lasso model predictions as new features to the training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Original training feature set.\n",
    "    - X_test (pd.DataFrame): Original testing feature set.\n",
    "    - y_train_pred_proba (np.array): Lasso predictions for the training set.\n",
    "    - y_test_pred_proba (np.array): Lasso predictions for the testing set.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_new (pd.DataFrame): Training set with Lasso predictions added.\n",
    "    - X_test_new (pd.DataFrame): Testing set with Lasso predictions added.\n",
    "    \"\"\"\n",
    "    # Create new feature columns\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    \n",
    "    X_train_new['lasso_pred_proba'] = y_train_pred_proba\n",
    "    X_test_new['lasso_pred_proba'] = y_test_pred_proba\n",
    "    \n",
    "    return X_train_new, X_test_new\n",
    "\n",
    "def evaluate_lightgbm_with_lasso(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a LightGBM model with Lasso predictions as an additional feature.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training feature set with Lasso predictions.\n",
    "    - y_train (pd.Series): Training target variable.\n",
    "    - X_test (pd.DataFrame): Testing feature set with Lasso predictions.\n",
    "    - y_test (pd.Series): Testing target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - model (lgb.LGBMClassifier): Trained LightGBM model.\n",
    "    - roc_auc (float): ROC-AUC score on the test set.\n",
    "    - report (str): Classification report with precision, recall, and F1-score up to 4 decimals.\n",
    "    \"\"\"\n",
    "    model = lgb.LGBMClassifier(\n",
    "        random_state=69,        # For reproducibility\n",
    "        n_jobs=-1,\n",
    "        force_col_wise=True     # Utilize all available cores\n",
    "    )\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities for the positive class\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Predict class labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate ROC-AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Generate classification report with 4 decimal places\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    return model, roc_auc, report\n",
    "\n",
    "def evaluate_l1_with_predictions(X_train, y_train, X_test, y_test, best_C):\n",
    "    \"\"\"\n",
    "    Performs feature selection using L1 Regularization (Lasso), adds Lasso predictions as a new feature,\n",
    "    and evaluates a LightGBM model.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training feature set (already scaled and encoded).\n",
    "    - y_train (pd.Series): Training target variable.\n",
    "    - X_test (pd.DataFrame): Testing feature set (already scaled and encoded).\n",
    "    - y_test (pd.Series): Testing target variable.\n",
    "    - best_C (float): The best C value identified from previous Lasso feature selection.\n",
    "    \n",
    "    Returns:\n",
    "    - model (lgb.LGBMClassifier): Trained LightGBM model.\n",
    "    - roc_auc (float): ROC-AUC score on the test set.\n",
    "    - report (str): Classification report with precision, recall, and F1-score up to 4 decimals.\n",
    "    \"\"\"\n",
    "    # Generate Lasso predictions\n",
    "    y_train_pred_proba, y_test_pred_proba = generate_lasso_predictions(X_train, y_train, X_test, best_C)\n",
    "    \n",
    "    # Add Lasso predictions as new features\n",
    "    X_train_new, X_test_new = add_lasso_predictions(X_train, X_test, y_train_pred_proba, y_test_pred_proba)\n",
    "    \n",
    "    # Evaluate LightGBM with the new feature\n",
    "    model, roc_auc, report = evaluate_lightgbm_with_lasso(X_train_new, y_train, X_test_new, y_test)\n",
    "    \n",
    "    return model, roc_auc, report\n",
    "\n",
    "# Example Data (Replace with your actual data)\n",
    "# X_train, X_test, y_train, y_test = your_data_splitting_function()\n",
    "\n",
    "# Assume you have already performed Lasso feature selection and identified best_C\n",
    "best_C = 1  # Replace with your actual best C value\n",
    "\n",
    "# Evaluate LightGBM with Lasso Predictions as an Additional Feature\n",
    "print(\"=== Evaluating LightGBM with Lasso Predictions ===\")\n",
    "model_lgbm_with_lasso, roc_auc_lgbm_lasso, report_lgbm_lasso = evaluate_l1_with_predictions(\n",
    "    X_train, y_train, X_test, y_test, best_C\n",
    ")\n",
    "\n",
    "print(\"\\n=== LightGBM with Lasso Predictions Classification Report ===\")\n",
    "print(report_lgbm_lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elasticnet_predictions(X_train, y_train, X_test, best_C, best_l1_ratio):\n",
    "    \"\"\"\n",
    "    Trains an Elastic Net model and generates predictions for training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training feature set (already scaled and encoded).\n",
    "    - y_train (pd.Series): Training target variable.\n",
    "    - X_test (pd.DataFrame): Testing feature set (already scaled and encoded).\n",
    "    - best_C (float): The best C value identified from previous feature selection.\n",
    "    - best_l1_ratio (float): The best l1_ratio value identified from previous feature selection.\n",
    "    \n",
    "    Returns:\n",
    "    - y_train_pred_proba (np.array): Predicted probabilities for the training set.\n",
    "    - y_test_pred_proba (np.array): Predicted probabilities for the testing set.\n",
    "    \"\"\"\n",
    "    # Initialize the Elastic Net model with the best C and l1_ratio\n",
    "    enet = LogisticRegression(\n",
    "        penalty='elasticnet',\n",
    "        C=best_C,\n",
    "        l1_ratio=best_l1_ratio,\n",
    "        solver='saga',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train the Elastic Net model on the entire training set\n",
    "    enet.fit(X_train, y_train)\n",
    "    \n",
    "    # Generate predicted probabilities for both training and testing sets\n",
    "    y_train_pred_proba = enet.predict_proba(X_train)[:, 1]\n",
    "    y_test_pred_proba = enet.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return y_train_pred_proba, y_test_pred_proba\n",
    "\n",
    "\n",
    "def add_elasticnet_predictions(X_train, X_test, y_train_pred_proba, y_test_pred_proba):\n",
    "    \"\"\"\n",
    "    Adds Elastic Net model predictions as new features to the training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Original training feature set.\n",
    "    - X_test (pd.DataFrame): Original testing feature set.\n",
    "    - y_train_pred_proba (np.array): Elastic Net predictions for the training set.\n",
    "    - y_test_pred_proba (np.array): Elastic Net predictions for the testing set.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_new (pd.DataFrame): Training set with Elastic Net predictions added.\n",
    "    - X_test_new (pd.DataFrame): Testing set with Elastic Net predictions added.\n",
    "    \"\"\"\n",
    "    # Create new feature columns\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    \n",
    "    X_train_new['elasticnet_pred_proba'] = y_train_pred_proba\n",
    "    X_test_new['elasticnet_pred_proba'] = y_test_pred_proba\n",
    "    \n",
    "    return X_train_new, X_test_new\n",
    "\n",
    "\n",
    "def evaluate_lightgbm_with_elasticnet(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a LightGBM model with Elastic Net predictions as an additional feature.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training feature set with Elastic Net predictions.\n",
    "    - y_train (pd.Series): Training target variable.\n",
    "    - X_test (pd.DataFrame): Testing feature set with Elastic Net predictions.\n",
    "    - y_test (pd.Series): Testing target variable.\n",
    "    \n",
    "    Returns:\n",
    "    - model (lgb.LGBMClassifier): Trained LightGBM model.\n",
    "    - roc_auc (float): ROC-AUC score on the test set.\n",
    "    - report (str): Classification report with precision, recall, and F1-score up to 4 decimals.\n",
    "    \"\"\"\n",
    "    model = lgb.LGBMClassifier(\n",
    "        random_state=69,        # For reproducibility\n",
    "        n_jobs=-1,\n",
    "        force_col_wise=True     # Utilize all available cores\n",
    "    )\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities for the positive class\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Predict class labels\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate ROC-AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Generate classification report with 4 decimal places\n",
    "    report = classification_report(y_test, y_pred, digits=4)\n",
    "    \n",
    "    # Print the evaluation metrics\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    return model, roc_auc, report\n",
    "\n",
    "def evaluate_elasticnet_with_predictions(X_train, y_train, X_test, y_test, best_C, best_l1_ratio):\n",
    "    \"\"\"\n",
    "    Performs feature selection using Elastic Net Regularization, adds Elastic Net predictions as a new feature,\n",
    "    and evaluates a LightGBM model.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training feature set (already scaled and encoded).\n",
    "    - y_train (pd.Series): Training target variable.\n",
    "    - X_test (pd.DataFrame): Testing feature set (already scaled and encoded).\n",
    "    - y_test (pd.Series): Testing target variable.\n",
    "    - best_C (float): The best C value identified from previous Elastic Net feature selection.\n",
    "    - best_l1_ratio (float): The best l1_ratio value identified from previous Elastic Net feature selection.\n",
    "    \n",
    "    Returns:\n",
    "    - model (lgb.LGBMClassifier): Trained LightGBM model.\n",
    "    - roc_auc (float): ROC-AUC score on the test set.\n",
    "    - report (str): Classification report with precision, recall, and F1-score up to 4 decimals.\n",
    "    \"\"\"\n",
    "    # Generate Elastic Net predictions\n",
    "    y_train_pred_proba, y_test_pred_proba = generate_elasticnet_predictions(\n",
    "        X_train, y_train, X_test, best_C, best_l1_ratio\n",
    "    )\n",
    "    \n",
    "    # Add Elastic Net predictions as new features\n",
    "    X_train_new, X_test_new = add_elasticnet_predictions(\n",
    "        X_train, X_test, y_train_pred_proba, y_test_pred_proba\n",
    "    )\n",
    "    \n",
    "    # Evaluate LightGBM with the new feature\n",
    "    model, roc_auc, report = evaluate_lightgbm_with_elasticnet(X_train_new, y_train, X_test_new, y_test)\n",
    "    \n",
    "    return model, roc_auc, report\n",
    "\n",
    "# Example Data (Replace with your actual data)\n",
    "# X_train, X_test, y_train, y_test = your_data_splitting_function()\n",
    "\n",
    "# Assume you have already performed Elastic Net feature selection and identified best_C and best_l1_ratio\n",
    "best_C = 1       # Replace with your actual best C value\n",
    "best_l1_ratio = 0.5  # Replace with your actual best l1_ratio value\n",
    "\n",
    "# Evaluate LightGBM with Elastic Net Predictions as an Additional Feature\n",
    "print(\"=== Evaluating LightGBM with Elastic Net Predictions ===\")\n",
    "model_lgbm_with_enet, roc_auc_lgbm_enet, report_lgbm_enet = evaluate_elasticnet_with_predictions(\n",
    "    X_train, y_train, X_test, y_test, best_C, best_l1_ratio\n",
    ")\n",
    "\n",
    "print(\"\\n=== LightGBM with Elastic Net Predictions Classification Report ===\")\n",
    "print(report_lgbm_enet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
